BNCparse
================

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

Quirin Würschinger, LMU Munich

<q.wuerschinger@lmu.de>

## Data overview

<div>

<p>

<img src="index_files/figure-commonmark/mermaid-figure-1.png"
style="width:7in;height:11.98in" />

</p>

</div>

# Load packages

Package requirements are stored in `requirements.yml`.

# Variables

BNC2014 needs to be downloaded for this script to work. It can be
obtained from the official [BNC
website](http://corpora.lancs.ac.uk/bnc2014/).

The following variables need to be updated to the corpus’ local path. In
the current setting the BNC2014 data were stored in the project folder
in the folder `data/bnc-2014-spoken`.

For development, I use a small subset of the corpus contained in
`data/test` that only contains the first 10 texts.

``` python
testing = True

if testing:
    path_bnc = Path('../data/test/bnc-2014-spoken')
    assert path_bnc.exists()
    texts_n = 10
    tokens_n = 94_659
else:
    path_bnc = Path('../data/bnc-2014-spoken')
    assert path_bnc.exists()
    texts_n = 1251
    tokens_n = 11_422_615
```

``` python
path_corpus = Path(path_bnc / 'spoken' / 'tagged')
assert path_corpus.exists()
path_metadata = Path(path_bnc / 'spoken' / 'metadata')
assert path_metadata.exists()
```

``` python
fp_meta_speakers = Path('../data/bnc-2014-spoken/spoken/metadata/bnc2014spoken-speakerdata.tsv')
assert fp_meta_speakers.exists()
fp_meta_speakers_fields = Path('../data/bnc-2014-spoken/spoken/metadata/metadata-fields-speaker.txt')
assert fp_meta_speakers_fields.exists()
fp_meta_texts = Path('../data/bnc-2014-spoken/spoken/metadata/bnc2014spoken-textdata.tsv')
assert fp_meta_texts.exists()
fp_meta_texts_fields = Path('../data/bnc-2014-spoken/spoken/metadata/metadata-fields-text.txt')
assert fp_meta_texts_fields.exists()
```

# Load and parse XML

``` python
path_texts = list(path_corpus.glob('*.xml'))
```

``` python
assert len(path_texts) == texts_n
```

------------------------------------------------------------------------

<a
href="https://github.com/wuqui/bncparse/blob/main/bncparse/core.py#L15"
target="_blank" style="float:right; font-size:smaller">source</a>

### get_xml

>      get_xml (f_path)

``` python
texts = [get_xml(path) for path in path_texts]
```

# Texts

``` python
meta_texts_head = pd.read_csv(
    fp_meta_texts_fields,
    delimiter='\t',
    skiprows=1,
    index_col=0
)
```

``` python
meta_texts = pd.read_csv(
    fp_meta_texts, 
    delimiter='\t', 
    names=meta_texts_head['XML tag'],
    index_col=0
)
```

## Add number of tokens per text

``` python
texts_tokens = []

for text in texts:
    text_d = {}
    text_d['text_id'] = text.get('id')
    text_d['text_toks_n'] = 0
    for tok in text.iter('w'):
        text_d['text_toks_n'] += 1
    texts_tokens.append(text_d)
```

``` python
texts_tokens = pd.DataFrame(texts_tokens)
texts_tokens
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text_id</th>
      <th>text_toks_n</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>S2EF</td>
      <td>16644</td>
    </tr>
    <tr>
      <th>1</th>
      <td>S2CY</td>
      <td>2706</td>
    </tr>
    <tr>
      <th>2</th>
      <td>S2AJ</td>
      <td>4161</td>
    </tr>
    <tr>
      <th>3</th>
      <td>S2B5</td>
      <td>7372</td>
    </tr>
    <tr>
      <th>4</th>
      <td>S2DD</td>
      <td>11452</td>
    </tr>
    <tr>
      <th>5</th>
      <td>S2A5</td>
      <td>1897</td>
    </tr>
    <tr>
      <th>6</th>
      <td>S2AX</td>
      <td>14492</td>
    </tr>
    <tr>
      <th>7</th>
      <td>S2E2</td>
      <td>4883</td>
    </tr>
    <tr>
      <th>8</th>
      <td>S2C9</td>
      <td>25593</td>
    </tr>
    <tr>
      <th>9</th>
      <td>S2FQ</td>
      <td>5459</td>
    </tr>
  </tbody>
</table>
</div>

``` python
# reset index and call it text_id
meta_texts_merge = meta_texts.reset_index().rename(columns={'index': 'text_id'})
```

``` python
meta_texts = pd.merge(
    left=meta_texts_merge,
    right=texts_tokens,
    on='text_id'
)
```

``` python
meta_texts
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text_id</th>
      <th>rec_length</th>
      <th>rec_date</th>
      <th>rec_year</th>
      <th>rec_period</th>
      <th>n_speakers</th>
      <th>list_speakers</th>
      <th>rec_loc</th>
      <th>relationships</th>
      <th>topics</th>
      <th>activity</th>
      <th>conv_type</th>
      <th>conventions</th>
      <th>in_sample</th>
      <th>transcriber</th>
      <th>text_toks_n</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>S2A5</td>
      <td>0:12:20</td>
      <td>2014-08-28</td>
      <td>2014</td>
      <td>2014_Q3</td>
      <td>2</td>
      <td>S0024 S0144</td>
      <td>Speakers' home</td>
      <td>Close family, partners, very close friends</td>
      <td>meeting; making arrangements for going to loca...</td>
      <td>Partners have a chat about jetlag and babies.</td>
      <td>Discussing</td>
      <td>Revised</td>
      <td>y</td>
      <td>T15</td>
      <td>1897</td>
    </tr>
    <tr>
      <th>1</th>
      <td>S2AJ</td>
      <td>0:19:24</td>
      <td>2015-08-04</td>
      <td>2015</td>
      <td>2015_Q3</td>
      <td>2</td>
      <td>S0439 S0441</td>
      <td>Home - kitchen</td>
      <td>Close family, partners, very close friends</td>
      <td>Food, old school friends, complaining about th...</td>
      <td>Catch-up with housemate.</td>
      <td>Discussing, explaining, inquiring, complaining...</td>
      <td>Revised</td>
      <td>y</td>
      <td>T19</td>
      <td>4161</td>
    </tr>
    <tr>
      <th>2</th>
      <td>S2AX</td>
      <td>1:03:29</td>
      <td>2012-04-01</td>
      <td>2012</td>
      <td>2012_Q2</td>
      <td>2</td>
      <td>S0037 S0115</td>
      <td>ANON and ANON’s home, Cambridge</td>
      <td>Close family, partners, very close friends</td>
      <td>Music, elitism, magazines, dreams, Christmas d...</td>
      <td>ANON and ANON talking while listening to the r...</td>
      <td>Discussing, explaining, anecdote telling</td>
      <td>Original</td>
      <td>y</td>
      <td>T15</td>
      <td>14492</td>
    </tr>
    <tr>
      <th>3</th>
      <td>S2B5</td>
      <td>0:36:03</td>
      <td>2012-03-06</td>
      <td>2012</td>
      <td>2012_Q1</td>
      <td>2</td>
      <td>S0024 S0144</td>
      <td>The Swan pub, Norfolk</td>
      <td>Close family, partners, very close friends</td>
      <td>Dogs, property, economics, health</td>
      <td>Husband and wife discuss some issues over a dr...</td>
      <td>Discussing, explaining</td>
      <td>Original</td>
      <td>y</td>
      <td>T20</td>
      <td>7372</td>
    </tr>
    <tr>
      <th>4</th>
      <td>S2C9</td>
      <td>2:12:08</td>
      <td>2015-02-24</td>
      <td>2015</td>
      <td>2015_Q1</td>
      <td>2</td>
      <td>S0336 S0362</td>
      <td>Speaker's home</td>
      <td>Friends, wider family circle</td>
      <td>Friends, family, work, holidays, festivals, ho...</td>
      <td>Friends catching up</td>
      <td>Discussing, explaining, inquiring, complaining...</td>
      <td>Revised</td>
      <td>n</td>
      <td>T10</td>
      <td>25593</td>
    </tr>
    <tr>
      <th>5</th>
      <td>S2CY</td>
      <td>0:14:25</td>
      <td>2015-11-27</td>
      <td>2015</td>
      <td>2015_Q4</td>
      <td>2</td>
      <td>S0679 S0680</td>
      <td>ANON’s living room, Leeds</td>
      <td>Close family, partners, very close friends</td>
      <td>Computers, Work colleagues in computing, Furni...</td>
      <td>Late evening chat</td>
      <td>Discussing, anecdote telling, making arrangements</td>
      <td>Revised</td>
      <td>n</td>
      <td>T04</td>
      <td>2706</td>
    </tr>
    <tr>
      <th>6</th>
      <td>S2DD</td>
      <td>1:04:04</td>
      <td>2016-06-21</td>
      <td>2016</td>
      <td>2016_Q2</td>
      <td>4</td>
      <td>S0687 S0688 S0689 S0690</td>
      <td>A restaurant, Istria, Croatia</td>
      <td>Close family, partners, very close friends</td>
      <td>Food, Drink, Weather, Cities and towns in Istr...</td>
      <td>NaN</td>
      <td>Discussing, explaining, inquiring</td>
      <td>Revised</td>
      <td>n</td>
      <td>T10</td>
      <td>11452</td>
    </tr>
    <tr>
      <th>7</th>
      <td>S2E2</td>
      <td>0:23:55</td>
      <td>2012-04-16</td>
      <td>2012</td>
      <td>2012_Q2</td>
      <td>2</td>
      <td>S0030 S0096</td>
      <td>The university, Salford</td>
      <td>Colleagues</td>
      <td>CVs</td>
      <td>Colleagues Talking about Writing a CV</td>
      <td>Discussing, explaining, advising</td>
      <td>Original</td>
      <td>y</td>
      <td>T11</td>
      <td>4883</td>
    </tr>
    <tr>
      <th>8</th>
      <td>S2EF</td>
      <td>1:25:40</td>
      <td>2016-01-10</td>
      <td>2016</td>
      <td>2016_Q1</td>
      <td>4</td>
      <td>S0567 S0611 S0620 S0623</td>
      <td>All speakers’ rented uni home, Lancaster</td>
      <td>Close family, partners, very close friends</td>
      <td>Discussing/eating food, bedrooms, maths, Sugar...</td>
      <td>Talking while eating pizza with housemates</td>
      <td>Discussing, explaining, inquiring, anecdote te...</td>
      <td>Revised</td>
      <td>n</td>
      <td>T10</td>
      <td>16644</td>
    </tr>
    <tr>
      <th>9</th>
      <td>S2FQ</td>
      <td>0:37:35</td>
      <td>2014-09-05</td>
      <td>2014</td>
      <td>2014_Q3</td>
      <td>2</td>
      <td>S0261 S0262</td>
      <td>House-sitting in Australia</td>
      <td>Close family, partners, very close friends</td>
      <td>social story-telling, problem-solving techniqu...</td>
      <td>A couple discussing their ideas for innovative...</td>
      <td>Discussing, explaining, inquiring, advising, r...</td>
      <td>Revised</td>
      <td>y</td>
      <td>T19</td>
      <td>5459</td>
    </tr>
  </tbody>
</table>
</div>

``` python
meta_texts.to_csv('../out/texts.csv', index=False)
```

# Utterances

``` python
utterances = []

for text in texts:
    for u in text.findall('u'):
        u_d = {}
        u_d['text_id'] = text.get('id')
        u_d['u_n'] = u.get('n')
        u_d['u_who'] = u.get('who')
        u_d['u_trans'] = u.get('trans')
        u_d['u_whoConfidence'] = u.get('whoConfidence')
        u_d['u_toks_n'] = len(list(u.iter('w')))
        utterances.append(u_d)
```

``` python
utterances = pd.DataFrame(utterances)
```

``` python
utterances
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text_id</th>
      <th>u_n</th>
      <th>u_who</th>
      <th>u_trans</th>
      <th>u_whoConfidence</th>
      <th>u_toks_n</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>S2EF</td>
      <td>1</td>
      <td>S0567</td>
      <td>nonoverlap</td>
      <td>high</td>
      <td>12</td>
    </tr>
    <tr>
      <th>1</th>
      <td>S2EF</td>
      <td>2</td>
      <td>S0623</td>
      <td>nonoverlap</td>
      <td>high</td>
      <td>8</td>
    </tr>
    <tr>
      <th>2</th>
      <td>S2EF</td>
      <td>3</td>
      <td>S0620</td>
      <td>nonoverlap</td>
      <td>high</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>S2EF</td>
      <td>4</td>
      <td>S0623</td>
      <td>nonoverlap</td>
      <td>high</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>S2EF</td>
      <td>5</td>
      <td>S0620</td>
      <td>nonoverlap</td>
      <td>high</td>
      <td>4</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>8803</th>
      <td>S2FQ</td>
      <td>239</td>
      <td>S0261</td>
      <td>nonoverlap</td>
      <td>high</td>
      <td>52</td>
    </tr>
    <tr>
      <th>8804</th>
      <td>S2FQ</td>
      <td>240</td>
      <td>S0262</td>
      <td>nonoverlap</td>
      <td>high</td>
      <td>12</td>
    </tr>
    <tr>
      <th>8805</th>
      <td>S2FQ</td>
      <td>241</td>
      <td>S0261</td>
      <td>nonoverlap</td>
      <td>high</td>
      <td>9</td>
    </tr>
    <tr>
      <th>8806</th>
      <td>S2FQ</td>
      <td>242</td>
      <td>S0262</td>
      <td>nonoverlap</td>
      <td>high</td>
      <td>7</td>
    </tr>
    <tr>
      <th>8807</th>
      <td>S2FQ</td>
      <td>243</td>
      <td>S0261</td>
      <td>nonoverlap</td>
      <td>high</td>
      <td>8</td>
    </tr>
  </tbody>
</table>
<p>8808 rows × 6 columns</p>
</div>

``` python
utterances.to_csv('../out/utterances.csv', index=False)
```

# Speakers

``` python
meta_speakers_head = pd.read_csv(
    fp_meta_speakers_fields,
    delimiter='\t',
    skiprows=1,
    index_col=0
)
```

``` python
meta_speakers = pd.read_csv(
    fp_meta_speakers, 
    delimiter='\t', 
    names=meta_speakers_head['XML tag'],
    index_col=0
)
```

``` python
meta_speakers
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>exactage</th>
      <th>age1994</th>
      <th>agerange</th>
      <th>gender</th>
      <th>nat</th>
      <th>birthplace</th>
      <th>birthcountry</th>
      <th>l1</th>
      <th>lingorig</th>
      <th>dialect_rep</th>
      <th>...</th>
      <th>dialect_l2</th>
      <th>dialect_l3</th>
      <th>dialect_l4</th>
      <th>edqual</th>
      <th>occupation</th>
      <th>socgrade</th>
      <th>nssec</th>
      <th>l2</th>
      <th>fls</th>
      <th>in_core</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>S0001</th>
      <td>32</td>
      <td>25_34</td>
      <td>30_39</td>
      <td>F</td>
      <td>British</td>
      <td>Wordsley, West Midlands</td>
      <td>England</td>
      <td>English</td>
      <td>England</td>
      <td>None indicated</td>
      <td>...</td>
      <td>unspecified</td>
      <td>unspecified</td>
      <td>unspecified</td>
      <td>5_postgrad</td>
      <td>University researcher</td>
      <td>A</td>
      <td>1_2</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>n</td>
    </tr>
    <tr>
      <th>S0002</th>
      <td>NaN</td>
      <td>Unknown</td>
      <td>19_29</td>
      <td>F</td>
      <td>British</td>
      <td>Birmingham</td>
      <td>England</td>
      <td>English</td>
      <td>England</td>
      <td>Midlands</td>
      <td>...</td>
      <td>england</td>
      <td>midlands</td>
      <td>unspecified</td>
      <td>5_postgrad</td>
      <td>Teacher</td>
      <td>B</td>
      <td>2</td>
      <td>NaN</td>
      <td>Japanese -- Intermediate</td>
      <td>n</td>
    </tr>
    <tr>
      <th>S0003</th>
      <td>NaN</td>
      <td>Unknown</td>
      <td>19_29</td>
      <td>F</td>
      <td>British</td>
      <td>Royal Leamington Spa, Warwickshire</td>
      <td>England</td>
      <td>English</td>
      <td>England</td>
      <td>Northern</td>
      <td>...</td>
      <td>england</td>
      <td>north</td>
      <td>unspecified</td>
      <td>4_graduate</td>
      <td>Student</td>
      <td>E</td>
      <td>uncat</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>n</td>
    </tr>
    <tr>
      <th>S0004</th>
      <td>NaN</td>
      <td>Unknown</td>
      <td>30_39</td>
      <td>M</td>
      <td>British</td>
      <td>NaN</td>
      <td>Germany</td>
      <td>English</td>
      <td>England</td>
      <td>Northern</td>
      <td>...</td>
      <td>england</td>
      <td>north</td>
      <td>unspecified</td>
      <td>5_postgrad</td>
      <td>Engineer</td>
      <td>C2</td>
      <td>5</td>
      <td>NaN</td>
      <td>Spanish -- Beginner</td>
      <td>n</td>
    </tr>
    <tr>
      <th>S0005</th>
      <td>NaN</td>
      <td>60plus</td>
      <td>80_89</td>
      <td>F</td>
      <td>British</td>
      <td>Birmingham</td>
      <td>England</td>
      <td>English</td>
      <td>England</td>
      <td>Midlands</td>
      <td>...</td>
      <td>england</td>
      <td>midlands</td>
      <td>unspecified</td>
      <td>2_secondary</td>
      <td>Insurance Broker (retired)</td>
      <td>E</td>
      <td>8</td>
      <td>NaN</td>
      <td>French -- Beginner</td>
      <td>n</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>S0691</th>
      <td>45</td>
      <td>45_59</td>
      <td>40_49</td>
      <td>F</td>
      <td>British</td>
      <td>Barrow-In-Furness</td>
      <td>UK</td>
      <td>English</td>
      <td>England</td>
      <td>Northern/ Cumbrian</td>
      <td>...</td>
      <td>england</td>
      <td>north</td>
      <td>unspecified</td>
      <td>3_sixthform</td>
      <td>dental nurse (trainee)</td>
      <td>D</td>
      <td>6</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>y</td>
    </tr>
    <tr>
      <th>S0692</th>
      <td>22</td>
      <td>15_24</td>
      <td>19_29</td>
      <td>M</td>
      <td>British</td>
      <td>Barrow-in-Furness</td>
      <td>England</td>
      <td>English</td>
      <td>England</td>
      <td>Northern</td>
      <td>...</td>
      <td>england</td>
      <td>north</td>
      <td>unspecified</td>
      <td>3_sixthform</td>
      <td>Sales Assistant (Part time)</td>
      <td>D</td>
      <td>6</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>n</td>
    </tr>
    <tr>
      <th>UNKFEMALE</th>
      <td>NaN</td>
      <td>Unknown</td>
      <td>Unknown</td>
      <td>F</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>None indicated</td>
      <td>...</td>
      <td>unspecified</td>
      <td>unspecified</td>
      <td>unspecified</td>
      <td>9_unknown</td>
      <td>NaN</td>
      <td>unknown</td>
      <td>unknown</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>n</td>
    </tr>
    <tr>
      <th>UNKMALE</th>
      <td>NaN</td>
      <td>Unknown</td>
      <td>Unknown</td>
      <td>M</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>None indicated</td>
      <td>...</td>
      <td>unspecified</td>
      <td>unspecified</td>
      <td>unspecified</td>
      <td>9_unknown</td>
      <td>NaN</td>
      <td>unknown</td>
      <td>unknown</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>n</td>
    </tr>
    <tr>
      <th>UNKMULTI</th>
      <td>NaN</td>
      <td>Unknown</td>
      <td>Unknown</td>
      <td>X</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>None indicated</td>
      <td>...</td>
      <td>unspecified</td>
      <td>unspecified</td>
      <td>unspecified</td>
      <td>9_unknown</td>
      <td>NaN</td>
      <td>unknown</td>
      <td>unknown</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>n</td>
    </tr>
  </tbody>
</table>
<p>671 rows × 24 columns</p>
</div>

## Add number of tokens per speaker

``` python
speakers_toks = defaultdict(int)

for text in texts:
    for u in text.iter('u'):
        who = u.get('who')
        n_words = len([w for w in u.iter('w')])
        speakers_toks[who] += n_words
```

``` python
speaker_toks = pd.DataFrame(list(speakers_toks.items()), columns=['who', 'speaker_toks_n'])
```

``` python
speaker_toks.sort_values(by='speaker_toks_n', ascending=False).head(10)
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>who</th>
      <th>speaker_toks_n</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>20</th>
      <td>S0336</td>
      <td>14332</td>
    </tr>
    <tr>
      <th>21</th>
      <td>S0362</td>
      <td>11261</td>
    </tr>
    <tr>
      <th>16</th>
      <td>S0115</td>
      <td>7522</td>
    </tr>
    <tr>
      <th>17</th>
      <td>S0037</td>
      <td>6970</td>
    </tr>
    <tr>
      <th>1</th>
      <td>S0623</td>
      <td>6621</td>
    </tr>
    <tr>
      <th>11</th>
      <td>S0024</td>
      <td>5086</td>
    </tr>
    <tr>
      <th>5</th>
      <td>S0611</td>
      <td>4698</td>
    </tr>
    <tr>
      <th>10</th>
      <td>S0144</td>
      <td>4183</td>
    </tr>
    <tr>
      <th>14</th>
      <td>S0687</td>
      <td>3810</td>
    </tr>
    <tr>
      <th>22</th>
      <td>S0261</td>
      <td>3596</td>
    </tr>
  </tbody>
</table>
</div>

``` python
meta_speakers_merge = meta_speakers.reset_index().rename(columns={'index': 'who'})
```

``` python
meta_speakers = pd.merge(
    left=meta_speakers_merge,
    right=speaker_toks,
    on='who'
)
```

``` python
meta_speakers
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>who</th>
      <th>exactage</th>
      <th>age1994</th>
      <th>agerange</th>
      <th>gender</th>
      <th>nat</th>
      <th>birthplace</th>
      <th>birthcountry</th>
      <th>l1</th>
      <th>lingorig</th>
      <th>...</th>
      <th>dialect_l3</th>
      <th>dialect_l4</th>
      <th>edqual</th>
      <th>occupation</th>
      <th>socgrade</th>
      <th>nssec</th>
      <th>l2</th>
      <th>fls</th>
      <th>in_core</th>
      <th>speaker_toks_n</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>S0024</td>
      <td>36</td>
      <td>35_44</td>
      <td>30_39</td>
      <td>F</td>
      <td>British</td>
      <td>Norwich</td>
      <td>England</td>
      <td>English</td>
      <td>England</td>
      <td>...</td>
      <td>south</td>
      <td>unspecified</td>
      <td>5_postgrad</td>
      <td>lecturer</td>
      <td>A</td>
      <td>1_2</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>n</td>
      <td>5086</td>
    </tr>
    <tr>
      <th>1</th>
      <td>S0030</td>
      <td>NaN</td>
      <td>Unknown</td>
      <td>40_49</td>
      <td>F</td>
      <td>British</td>
      <td>London</td>
      <td>England</td>
      <td>English</td>
      <td>England</td>
      <td>...</td>
      <td>south</td>
      <td>unspecified</td>
      <td>5_postgrad</td>
      <td>Careers Consultant</td>
      <td>B</td>
      <td>2</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>n</td>
      <td>2622</td>
    </tr>
    <tr>
      <th>2</th>
      <td>S0037</td>
      <td>NaN</td>
      <td>Unknown</td>
      <td>19_29</td>
      <td>F</td>
      <td>British</td>
      <td>Sunderland, Tyne and Wear</td>
      <td>England</td>
      <td>English</td>
      <td>England</td>
      <td>...</td>
      <td>north</td>
      <td>northeast</td>
      <td>5_postgrad</td>
      <td>Research Manager</td>
      <td>A</td>
      <td>1_2</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>n</td>
      <td>6970</td>
    </tr>
    <tr>
      <th>3</th>
      <td>S0096</td>
      <td>NaN</td>
      <td>Unknown</td>
      <td>30_39</td>
      <td>F</td>
      <td>British</td>
      <td>York</td>
      <td>England</td>
      <td>English</td>
      <td>England</td>
      <td>...</td>
      <td>north</td>
      <td>unspecified</td>
      <td>5_postgrad</td>
      <td>Careers Consultant</td>
      <td>B</td>
      <td>2</td>
      <td>Kutchi</td>
      <td>NaN</td>
      <td>n</td>
      <td>2261</td>
    </tr>
    <tr>
      <th>4</th>
      <td>S0115</td>
      <td>NaN</td>
      <td>Unknown</td>
      <td>30_39</td>
      <td>M</td>
      <td>British</td>
      <td>Birmingham</td>
      <td>England</td>
      <td>English</td>
      <td>England</td>
      <td>...</td>
      <td>midlands</td>
      <td>unspecified</td>
      <td>5_postgrad</td>
      <td>PhD student</td>
      <td>A</td>
      <td>1_2</td>
      <td>NaN</td>
      <td>French -- Advanced; German -- Advanced</td>
      <td>n</td>
      <td>7522</td>
    </tr>
    <tr>
      <th>5</th>
      <td>S0144</td>
      <td>36</td>
      <td>35_44</td>
      <td>30_39</td>
      <td>M</td>
      <td>British</td>
      <td>London</td>
      <td>England</td>
      <td>English</td>
      <td>England</td>
      <td>...</td>
      <td>south</td>
      <td>unspecified</td>
      <td>5_postgrad</td>
      <td>Lecturer</td>
      <td>A</td>
      <td>1_2</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>y</td>
      <td>4183</td>
    </tr>
    <tr>
      <th>6</th>
      <td>S0261</td>
      <td>41</td>
      <td>35_44</td>
      <td>40_49</td>
      <td>M</td>
      <td>British/New Zealand</td>
      <td>Wellington</td>
      <td>New Zealand</td>
      <td>English</td>
      <td>England/NZ</td>
      <td>...</td>
      <td>non_uk</td>
      <td>non_uk</td>
      <td>4_graduate</td>
      <td>Entrepreneur</td>
      <td>A</td>
      <td>1_2</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>n</td>
      <td>3596</td>
    </tr>
    <tr>
      <th>7</th>
      <td>S0262</td>
      <td>41</td>
      <td>35_44</td>
      <td>40_49</td>
      <td>F</td>
      <td>British</td>
      <td>Dorchester</td>
      <td>England</td>
      <td>English</td>
      <td>England</td>
      <td>...</td>
      <td>south</td>
      <td>unspecified</td>
      <td>5_postgrad</td>
      <td>teacher</td>
      <td>B</td>
      <td>2</td>
      <td>NaN</td>
      <td>French -- level unspecified; German -- level u...</td>
      <td>y</td>
      <td>1863</td>
    </tr>
    <tr>
      <th>8</th>
      <td>S0336</td>
      <td>24</td>
      <td>15_24</td>
      <td>19_29</td>
      <td>F</td>
      <td>British</td>
      <td>Wegberg</td>
      <td>Germany</td>
      <td>English</td>
      <td>England</td>
      <td>...</td>
      <td>unspecified</td>
      <td>unspecified</td>
      <td>5_postgrad</td>
      <td>Administrator</td>
      <td>C1</td>
      <td>3</td>
      <td>NaN</td>
      <td>German -- Beginner</td>
      <td>n</td>
      <td>14332</td>
    </tr>
    <tr>
      <th>9</th>
      <td>S0362</td>
      <td>25</td>
      <td>25_34</td>
      <td>19_29</td>
      <td>M</td>
      <td>British</td>
      <td>Leicester</td>
      <td>England</td>
      <td>English</td>
      <td>England</td>
      <td>...</td>
      <td>unspecified</td>
      <td>unspecified</td>
      <td>5_postgrad</td>
      <td>Coach/Franchise owner</td>
      <td>B</td>
      <td>2</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>y</td>
      <td>11261</td>
    </tr>
    <tr>
      <th>10</th>
      <td>S0439</td>
      <td>23</td>
      <td>15_24</td>
      <td>19_29</td>
      <td>F</td>
      <td>British</td>
      <td>UK</td>
      <td>UK</td>
      <td>English</td>
      <td>England</td>
      <td>...</td>
      <td>south</td>
      <td>unspecified</td>
      <td>4_graduate</td>
      <td>Project Co-ordinator</td>
      <td>B</td>
      <td>2</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>n</td>
      <td>2032</td>
    </tr>
    <tr>
      <th>11</th>
      <td>S0441</td>
      <td>24</td>
      <td>15_24</td>
      <td>19_29</td>
      <td>F</td>
      <td>British</td>
      <td>UK</td>
      <td>UK</td>
      <td>English</td>
      <td>England</td>
      <td>...</td>
      <td>south</td>
      <td>unspecified</td>
      <td>5_postgrad</td>
      <td>Mental Health Nurse</td>
      <td>B</td>
      <td>2</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>y</td>
      <td>2129</td>
    </tr>
    <tr>
      <th>12</th>
      <td>S0567</td>
      <td>20</td>
      <td>15_24</td>
      <td>19_29</td>
      <td>F</td>
      <td>British</td>
      <td>Birmingham</td>
      <td>UK</td>
      <td>English</td>
      <td>England</td>
      <td>...</td>
      <td>south</td>
      <td>unspecified</td>
      <td>4_graduate</td>
      <td>student</td>
      <td>E</td>
      <td>uncat</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>n</td>
      <td>2784</td>
    </tr>
    <tr>
      <th>13</th>
      <td>S0611</td>
      <td>20</td>
      <td>15_24</td>
      <td>19_29</td>
      <td>F</td>
      <td>British</td>
      <td>Wakefield, West Yorkshire</td>
      <td>England</td>
      <td>English</td>
      <td>England</td>
      <td>...</td>
      <td>north</td>
      <td>yorkshire</td>
      <td>4_graduate</td>
      <td>student</td>
      <td>E</td>
      <td>uncat</td>
      <td>NaN</td>
      <td>Spanish -- basic</td>
      <td>n</td>
      <td>4698</td>
    </tr>
    <tr>
      <th>14</th>
      <td>S0620</td>
      <td>19</td>
      <td>15_24</td>
      <td>19_29</td>
      <td>F</td>
      <td>British</td>
      <td>Stockport, UK</td>
      <td>England</td>
      <td>English</td>
      <td>England</td>
      <td>...</td>
      <td>unspecified</td>
      <td>unspecified</td>
      <td>3_sixthform</td>
      <td>Student</td>
      <td>E</td>
      <td>uncat</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>n</td>
      <td>2488</td>
    </tr>
    <tr>
      <th>15</th>
      <td>S0623</td>
      <td>20</td>
      <td>15_24</td>
      <td>19_29</td>
      <td>F</td>
      <td>British</td>
      <td>Manchester</td>
      <td>England</td>
      <td>English</td>
      <td>England</td>
      <td>...</td>
      <td>north</td>
      <td>northwest</td>
      <td>4_graduate</td>
      <td>student</td>
      <td>E</td>
      <td>uncat</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>y</td>
      <td>6621</td>
    </tr>
    <tr>
      <th>16</th>
      <td>S0679</td>
      <td>54</td>
      <td>45_59</td>
      <td>50_59</td>
      <td>F</td>
      <td>British</td>
      <td>Kent, Margate</td>
      <td>England</td>
      <td>English</td>
      <td>England</td>
      <td>...</td>
      <td>south</td>
      <td>unspecified</td>
      <td>5_postgrad</td>
      <td>Retired (University manager)</td>
      <td>E</td>
      <td>8</td>
      <td>NaN</td>
      <td>French -- level unspecified</td>
      <td>y</td>
      <td>848</td>
    </tr>
    <tr>
      <th>17</th>
      <td>S0680</td>
      <td>64</td>
      <td>60plus</td>
      <td>60_69</td>
      <td>F</td>
      <td>British</td>
      <td>Margate, Kent</td>
      <td>England</td>
      <td>English</td>
      <td>England</td>
      <td>...</td>
      <td>south</td>
      <td>unspecified</td>
      <td>4_graduate</td>
      <td>Retired teacher</td>
      <td>E</td>
      <td>8</td>
      <td>NaN</td>
      <td>French -- School</td>
      <td>y</td>
      <td>1858</td>
    </tr>
    <tr>
      <th>18</th>
      <td>S0687</td>
      <td>21</td>
      <td>15_24</td>
      <td>19_29</td>
      <td>F</td>
      <td>British</td>
      <td>Bolton</td>
      <td>England</td>
      <td>English</td>
      <td>UK</td>
      <td>...</td>
      <td>north</td>
      <td>northwest</td>
      <td>4_graduate</td>
      <td>Student</td>
      <td>E</td>
      <td>uncat</td>
      <td>NaN</td>
      <td>Spanish -- A-level</td>
      <td>y</td>
      <td>3810</td>
    </tr>
    <tr>
      <th>19</th>
      <td>S0688</td>
      <td>53</td>
      <td>45_59</td>
      <td>50_59</td>
      <td>F</td>
      <td>British</td>
      <td>Bolton</td>
      <td>England</td>
      <td>English</td>
      <td>UK</td>
      <td>...</td>
      <td>north</td>
      <td>northwest</td>
      <td>4_graduate</td>
      <td>Midwife</td>
      <td>B</td>
      <td>2</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>y</td>
      <td>2987</td>
    </tr>
    <tr>
      <th>20</th>
      <td>S0689</td>
      <td>16</td>
      <td>15_24</td>
      <td>11_18</td>
      <td>M</td>
      <td>British</td>
      <td>Bolton</td>
      <td>England</td>
      <td>English</td>
      <td>UK</td>
      <td>...</td>
      <td>north</td>
      <td>northwest</td>
      <td>2_secondary</td>
      <td>Student</td>
      <td>E</td>
      <td>uncat</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>y</td>
      <td>2832</td>
    </tr>
    <tr>
      <th>21</th>
      <td>S0690</td>
      <td>53</td>
      <td>45_59</td>
      <td>50_59</td>
      <td>M</td>
      <td>British</td>
      <td>London</td>
      <td>England</td>
      <td>English</td>
      <td>UK</td>
      <td>...</td>
      <td>south</td>
      <td>unspecified</td>
      <td>4_graduate</td>
      <td>Management Consultant</td>
      <td>A</td>
      <td>1_2</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>n</td>
      <td>1782</td>
    </tr>
    <tr>
      <th>22</th>
      <td>UNKFEMALE</td>
      <td>NaN</td>
      <td>Unknown</td>
      <td>Unknown</td>
      <td>F</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>unspecified</td>
      <td>unspecified</td>
      <td>9_unknown</td>
      <td>NaN</td>
      <td>unknown</td>
      <td>unknown</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>n</td>
      <td>91</td>
    </tr>
    <tr>
      <th>23</th>
      <td>UNKMULTI</td>
      <td>NaN</td>
      <td>Unknown</td>
      <td>Unknown</td>
      <td>X</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>unspecified</td>
      <td>unspecified</td>
      <td>9_unknown</td>
      <td>NaN</td>
      <td>unknown</td>
      <td>unknown</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>n</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
<p>24 rows × 26 columns</p>
</div>

## Write out

``` python
meta_speakers.to_csv('../out/speakers.csv', index=False)
```

# Tokens

In addition to the metadata present in the corpus, I’ve added the
following columns:

- `w_idx`: token position (‘index’) in the given utterance, starting at
  1
- `w_L1`: preceding token
- `w_R1`: subsequent token

``` python
tokens = []

for text in texts:
    tok_d = {}
    tok_d['text_id'] = text.get('id')

    for u in text.findall('u'):
        tok_d['u_n'] = u.get('n')

        u_toks = list(u.iter('w'))
        for i, w in enumerate(u_toks):
            tok_d['w_pos'] = w.get('pos')
            tok_d['w_lemma'] = w.get('lemma')
            tok_d['w_class'] = w.get('class')
            tok_d['w_usas'] = w.get('usas')
            tok_d['w_text'] = w.text
            tok_d['w_idx'] = i + 1
            tok_d['w_L1'] = u_toks[i-1].text if i > 0 else '<s>'
            tok_d['w_R1'] = u_toks[i+1].text if i < len(u_toks) - 1 else '</s>'

            tokens.append(tok_d.copy())
```

``` python
tokens = pd.DataFrame(tokens)
```

``` python
tokens.head(20)
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text_id</th>
      <th>u_n</th>
      <th>w_pos</th>
      <th>w_lemma</th>
      <th>w_class</th>
      <th>w_usas</th>
      <th>w_text</th>
      <th>w_idx</th>
      <th>w_L1</th>
      <th>w_R1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>S2EF</td>
      <td>1</td>
      <td>VM</td>
      <td>shall</td>
      <td>VERB</td>
      <td>T1:1:3</td>
      <td>shall</td>
      <td>1</td>
      <td>&lt;s&gt;</td>
      <td>I</td>
    </tr>
    <tr>
      <th>1</th>
      <td>S2EF</td>
      <td>1</td>
      <td>PPIS1</td>
      <td>i</td>
      <td>PRON</td>
      <td>Z8</td>
      <td>I</td>
      <td>2</td>
      <td>shall</td>
      <td>move</td>
    </tr>
    <tr>
      <th>2</th>
      <td>S2EF</td>
      <td>1</td>
      <td>VVI</td>
      <td>move</td>
      <td>VERB</td>
      <td>M2</td>
      <td>move</td>
      <td>3</td>
      <td>I</td>
      <td>the</td>
    </tr>
    <tr>
      <th>3</th>
      <td>S2EF</td>
      <td>1</td>
      <td>AT</td>
      <td>the</td>
      <td>ART</td>
      <td>Z5</td>
      <td>the</td>
      <td>4</td>
      <td>move</td>
      <td>laptops</td>
    </tr>
    <tr>
      <th>4</th>
      <td>S2EF</td>
      <td>1</td>
      <td>NN2</td>
      <td>laptop</td>
      <td>SUBST</td>
      <td>Y2</td>
      <td>laptops</td>
      <td>5</td>
      <td>the</td>
      <td>then</td>
    </tr>
    <tr>
      <th>5</th>
      <td>S2EF</td>
      <td>1</td>
      <td>RT</td>
      <td>then</td>
      <td>ADV</td>
      <td>N4</td>
      <td>then</td>
      <td>6</td>
      <td>laptops</td>
      <td>stick</td>
    </tr>
    <tr>
      <th>6</th>
      <td>S2EF</td>
      <td>1</td>
      <td>VV0</td>
      <td>stick</td>
      <td>VERB</td>
      <td>M2</td>
      <td>stick</td>
      <td>7</td>
      <td>then</td>
      <td>it</td>
    </tr>
    <tr>
      <th>7</th>
      <td>S2EF</td>
      <td>1</td>
      <td>PPH1</td>
      <td>it</td>
      <td>PRON</td>
      <td>Z8</td>
      <td>it</td>
      <td>8</td>
      <td>stick</td>
      <td>on</td>
    </tr>
    <tr>
      <th>8</th>
      <td>S2EF</td>
      <td>1</td>
      <td>II</td>
      <td>on</td>
      <td>PREP</td>
      <td>N6</td>
      <td>on</td>
      <td>9</td>
      <td>it</td>
      <td>the</td>
    </tr>
    <tr>
      <th>9</th>
      <td>S2EF</td>
      <td>1</td>
      <td>AT</td>
      <td>the</td>
      <td>ART</td>
      <td>N6</td>
      <td>the</td>
      <td>10</td>
      <td>on</td>
      <td>table</td>
    </tr>
    <tr>
      <th>10</th>
      <td>S2EF</td>
      <td>1</td>
      <td>NN1</td>
      <td>table</td>
      <td>SUBST</td>
      <td>N6</td>
      <td>table</td>
      <td>11</td>
      <td>the</td>
      <td>?</td>
    </tr>
    <tr>
      <th>11</th>
      <td>S2EF</td>
      <td>1</td>
      <td>YQUE</td>
      <td>PUNC</td>
      <td>STOP</td>
      <td></td>
      <td>?</td>
      <td>12</td>
      <td>table</td>
      <td>&lt;/s&gt;</td>
    </tr>
    <tr>
      <th>12</th>
      <td>S2EF</td>
      <td>2</td>
      <td>UH</td>
      <td>yeah</td>
      <td>INTERJ</td>
      <td>Z4</td>
      <td>yeah</td>
      <td>1</td>
      <td>&lt;s&gt;</td>
      <td>do</td>
    </tr>
    <tr>
      <th>13</th>
      <td>S2EF</td>
      <td>2</td>
      <td>VD0</td>
      <td>do</td>
      <td>VERB</td>
      <td>Z5</td>
      <td>do</td>
      <td>2</td>
      <td>yeah</td>
      <td>we</td>
    </tr>
    <tr>
      <th>14</th>
      <td>S2EF</td>
      <td>2</td>
      <td>PPIS2</td>
      <td>we</td>
      <td>PRON</td>
      <td>Z8</td>
      <td>we</td>
      <td>3</td>
      <td>do</td>
      <td>want</td>
    </tr>
    <tr>
      <th>15</th>
      <td>S2EF</td>
      <td>2</td>
      <td>VVI</td>
      <td>want</td>
      <td>VERB</td>
      <td>X7</td>
      <td>want</td>
      <td>4</td>
      <td>we</td>
      <td>plates</td>
    </tr>
    <tr>
      <th>16</th>
      <td>S2EF</td>
      <td>2</td>
      <td>NN2</td>
      <td>plate</td>
      <td>SUBST</td>
      <td>O2</td>
      <td>plates</td>
      <td>5</td>
      <td>want</td>
      <td>or</td>
    </tr>
    <tr>
      <th>17</th>
      <td>S2EF</td>
      <td>2</td>
      <td>CC</td>
      <td>or</td>
      <td>CONJ</td>
      <td>Z5</td>
      <td>or</td>
      <td>6</td>
      <td>plates</td>
      <td>no</td>
    </tr>
    <tr>
      <th>18</th>
      <td>S2EF</td>
      <td>2</td>
      <td>UH</td>
      <td>no</td>
      <td>INTERJ</td>
      <td>Z4</td>
      <td>no</td>
      <td>7</td>
      <td>or</td>
      <td>?</td>
    </tr>
    <tr>
      <th>19</th>
      <td>S2EF</td>
      <td>2</td>
      <td>YQUE</td>
      <td>PUNC</td>
      <td>STOP</td>
      <td></td>
      <td>?</td>
      <td>8</td>
      <td>no</td>
      <td>&lt;/s&gt;</td>
    </tr>
  </tbody>
</table>
</div>

``` python
assert len(tokens) == tokens_n
```

I export the full token table to `tokens.csv`.

``` python
if not testing:
    tokens.to_csv('../out/tokens.csv', index=False)
```

I also export a smaller version for use in spreadsheet software. This
version contains the first 50,000 tokens in the corpus and is stored in
`tokens_50k.csv`.

``` python
if not testing:
    (tokens
     .head(50_000)
     .to_csv('../out/tokens_50k.csv', index=False))
```

# Corpus statistics

## Texts

Calculate the total number of texts in the corpus.

``` python
text_ids = [xml.get('id') for xml in texts]

print(f"number of documents in the corpus: {len(text_ids)}")
```

    number of documents in the corpus: 10

``` python
assert len(text_ids) == texts_n
```

## Vocabulary

``` python
toks = []
for text in texts:
    for w in text.iter('w'):
        toks.append(w.text)
```

``` python
n_toks_types = pd.DataFrame(
    {'tokens': f'{len(tokens):,}', 
    'types': f'{len(set(tokens)):,}'}, 
    index=[0]
)

n_toks_types
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>tokens</th>
      <th>types</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>94,659</td>
      <td>8</td>
    </tr>
  </tbody>
</table>
</div>

# Merge tokens with metadata

``` python
tokens.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 94659 entries, 0 to 94658
    Data columns (total 8 columns):
     #   Column   Non-Null Count  Dtype 
    ---  ------   --------------  ----- 
     0   text_id  94659 non-null  object
     1   u_n      94659 non-null  object
     2   w_pos    94659 non-null  object
     3   w_lemma  94659 non-null  object
     4   w_class  94659 non-null  object
     5   w_usas   94659 non-null  object
     6   w_text   94659 non-null  object
     7   w_idx    94659 non-null  int64 
    dtypes: int64(1), object(7)
    memory usage: 5.8+ MB

## + utterance information

``` python
toks_utt = pd.merge(
    tokens,
    utterances,
    on = ['text_id', 'u_n']
)
```

``` python
toks_utt.info()
```

    <class 'pandas.core.frame.DataFrame'>
    Int64Index: 94659 entries, 0 to 94658
    Data columns (total 12 columns):
     #   Column           Non-Null Count  Dtype 
    ---  ------           --------------  ----- 
     0   text_id          94659 non-null  object
     1   u_n              94659 non-null  object
     2   w_pos            94659 non-null  object
     3   w_lemma          94659 non-null  object
     4   w_class          94659 non-null  object
     5   w_usas           94659 non-null  object
     6   w_text           94659 non-null  object
     7   w_idx            94659 non-null  int64 
     8   u_who            94659 non-null  object
     9   u_trans          94659 non-null  object
     10  u_whoConfidence  94659 non-null  object
     11  u_toks_n         94659 non-null  int64 
    dtypes: int64(2), object(10)
    memory usage: 9.4+ MB

## + text information

``` python
toks_utt_text = pd.merge(
    toks_utt,
    meta_texts,
    on = 'text_id'
)
```

``` python
toks_utt_text.info()
```

    <class 'pandas.core.frame.DataFrame'>
    Int64Index: 94659 entries, 0 to 94658
    Data columns (total 27 columns):
     #   Column           Non-Null Count  Dtype 
    ---  ------           --------------  ----- 
     0   text_id          94659 non-null  object
     1   u_n              94659 non-null  object
     2   w_pos            94659 non-null  object
     3   w_lemma          94659 non-null  object
     4   w_class          94659 non-null  object
     5   w_usas           94659 non-null  object
     6   w_text           94659 non-null  object
     7   w_idx            94659 non-null  int64 
     8   u_who            94659 non-null  object
     9   u_trans          94659 non-null  object
     10  u_whoConfidence  94659 non-null  object
     11  u_toks_n         94659 non-null  int64 
     12  rec_length       94659 non-null  object
     13  rec_date         94659 non-null  object
     14  rec_year         94659 non-null  int64 
     15  rec_period       94659 non-null  object
     16  n_speakers       94659 non-null  int64 
     17  list_speakers    94659 non-null  object
     18  rec_loc          94659 non-null  object
     19  relationships    94659 non-null  object
     20  topics           94659 non-null  object
     21  activity         83207 non-null  object
     22  conv_type        94659 non-null  object
     23  conventions      94659 non-null  object
     24  in_sample        94659 non-null  object
     25  transcriber      94659 non-null  object
     26  text_toks_n      94659 non-null  int64 
    dtypes: int64(5), object(22)
    memory usage: 20.2+ MB

## + speaker information

``` python
toks_utt_text_speakers = pd.merge(
    toks_utt_text,
    meta_speakers,
    left_on = 'u_who',
    right_on = 'who'
)
```

``` python
toks_utt_text_speakers.info()
```

    <class 'pandas.core.frame.DataFrame'>
    Int64Index: 94659 entries, 0 to 94658
    Data columns (total 53 columns):
     #   Column           Non-Null Count  Dtype 
    ---  ------           --------------  ----- 
     0   text_id          94659 non-null  object
     1   u_n              94659 non-null  object
     2   w_pos            94659 non-null  object
     3   w_lemma          94659 non-null  object
     4   w_class          94659 non-null  object
     5   w_usas           94659 non-null  object
     6   w_text           94659 non-null  object
     7   w_idx            94659 non-null  int64 
     8   u_who            94659 non-null  object
     9   u_trans          94659 non-null  object
     10  u_whoConfidence  94659 non-null  object
     11  u_toks_n         94659 non-null  int64 
     12  rec_length       94659 non-null  object
     13  rec_date         94659 non-null  object
     14  rec_year         94659 non-null  int64 
     15  rec_period       94659 non-null  object
     16  n_speakers       94659 non-null  int64 
     17  list_speakers    94659 non-null  object
     18  rec_loc          94659 non-null  object
     19  relationships    94659 non-null  object
     20  topics           94659 non-null  object
     21  activity         83207 non-null  object
     22  conv_type        94659 non-null  object
     23  conventions      94659 non-null  object
     24  in_sample        94659 non-null  object
     25  transcriber      94659 non-null  object
     26  text_toks_n      94659 non-null  int64 
     27  who              94659 non-null  object
     28  exactage         75190 non-null  object
     29  age1994          94659 non-null  object
     30  agerange         94659 non-null  object
     31  gender           94659 non-null  object
     32  nat              94565 non-null  object
     33  birthplace       94565 non-null  object
     34  birthcountry     94565 non-null  object
     35  l1               94565 non-null  object
     36  lingorig         94565 non-null  object
     37  dialect_rep      94659 non-null  object
     38  hab_city         89106 non-null  object
     39  hab_country      94565 non-null  object
     40  hab_dur          94565 non-null  object
     41  dialect_l1       94659 non-null  object
     42  dialect_l2       94659 non-null  object
     43  dialect_l3       94659 non-null  object
     44  dialect_l4       94659 non-null  object
     45  edqual           94659 non-null  object
     46  occupation       94565 non-null  object
     47  socgrade         94659 non-null  object
     48  nssec            94659 non-null  object
     49  l2               2261 non-null   object
     50  fls              34931 non-null  object
     51  in_core          94659 non-null  object
     52  speaker_toks_n   94659 non-null  int64 
    dtypes: int64(6), object(47)
    memory usage: 39.0+ MB

## Write out

``` python
toks_utt_text_speakers.to_csv('../out/tokens-plus-meta.csv', index=False)
```

``` python
print(f'number of rows: {len(toks_utt_text_speakers)}')
print(f'file size: {os.path.getsize("../out/tokens-plus-meta.csv") / 1_000_000:.2f} MB')
```

    number of rows: 94659
    file size: 64.40 MB

I also write out a small version containing the first 50,000 rows for
use in spreadsheet software:

``` python
toks_utt_text_speakers.iloc[:50_000].to_csv('../out/tokens-plus-meta_small.csv', index=False)
```

``` python
print(f'number of rows: {len(toks_utt_text_speakers.iloc[:50_000])}')
print(f'file size: {os.path.getsize("../out/tokens-plus-meta_small.csv") / 1_000_000:.2f} MB')
```

    number of rows: 50000
    file size: 40.04 MB
