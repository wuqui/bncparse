BNCparse
================

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

Quirin Würschinger, LMU Munich

<q.wuerschinger@lmu.de>

Documentation: <https://wuqui.github.io/bncparse/>

Please visit the above website, as GitHub cannot render everything
properly in the version below.

## Data overview

The diagram below illustrates all of the data that is currently
available. Variables that have been added to what was available from the
downloadable version of the BNC are marked with a `+` prefix.

<div>

<p>

<img src="index_files/figure-commonmark/mermaid-figure-1.png"
style="width:7in;height:8.14in" />

</p>

</div>

# Load packages

Package requirements are stored in `requirements.yml`.

# Variables

For development, I use a small subset of the corpus contained in
`data/test` that only contains the first 10 texts.

``` python
testing = True

if testing:
    path_bnc = Path('../data/test/bnc-2014-spoken')
    assert path_bnc.exists()
    texts_n = 10
    tokens_n = 94_659
else:
    path_bnc = Path('../data/bnc-2014-spoken')
    assert path_bnc.exists()
    texts_n = 1251
    tokens_n = 11_422_615
```

``` python
path_corpus = Path(path_bnc / 'spoken' / 'tagged')
path_corpus_untagged = Path(path_bnc / 'spoken' / 'untagged')
path_metadata = Path(path_bnc / 'spoken' / 'metadata')
fp_meta_speakers = Path('../data/bnc-2014-spoken/spoken/metadata/bnc2014spoken-speakerdata.tsv')
fp_meta_speakers_fields = Path('../data/bnc-2014-spoken/spoken/metadata/metadata-fields-speaker.txt')
fp_meta_texts = Path('../data/bnc-2014-spoken/spoken/metadata/bnc2014spoken-textdata.tsv')
fp_meta_texts_fields = Path('../data/bnc-2014-spoken/spoken/metadata/metadata-fields-text.txt')
```

``` python
assert path_corpus.exists()
assert path_corpus_untagged.exists()
assert path_metadata.exists()
assert fp_meta_speakers.exists()
assert fp_meta_speakers_fields.exists()
assert fp_meta_texts.exists()
assert fp_meta_texts_fields.exists()
```

# Load and parse XML

``` python
path_texts = list(path_corpus.glob('*.xml'))
```

``` python
assert len(path_texts) == texts_n
```

------------------------------------------------------------------------

<a
href="https://github.com/wuqui/bncparse/blob/main/bncparse/core.py#L16"
target="_blank" style="float:right; font-size:smaller">source</a>

### get_xml

>      get_xml (f_path)

``` python
texts = [get_xml(path) for path in path_texts]
```

# Texts

``` python
meta_texts_head = pd.read_csv(
    fp_meta_texts_fields,
    delimiter='\t',
    skiprows=1,
    index_col=0
)
```

``` python
meta_texts = pd.read_csv(
    fp_meta_texts, 
    delimiter='\t', 
    names=meta_texts_head['XML tag'],
    index_col=0
)
```

## Add number of tokens per text

``` python
texts_tokens = []

for text in texts:
    text_d = {}
    text_d['text_id'] = text.get('id')
    text_d['text_toks_n'] = 0
    for tok in text.iter('w'):
        text_d['text_toks_n'] += 1
    texts_tokens.append(text_d)
```

``` python
texts_tokens = pd.DataFrame(texts_tokens)
texts_tokens
```

``` python
# reset index and call it text_id
meta_texts_merge = meta_texts.reset_index().rename(columns={'index': 'text_id'})
```

``` python
meta_texts = pd.merge(
    left=meta_texts_merge,
    right=texts_tokens,
    on='text_id'
)
```

``` python
meta_texts
```

``` python
if not testing:
    meta_texts.to_csv('../out/texts.csv', index=False)
```

# Utterances

``` python
utterances = []

for text in texts:
    for u in text.findall('u'):
        u_d = {}
        u_d['text_id'] = text.get('id')
        u_d['u_n'] = u.get('n')
        u_d['u_who'] = u.get('who')
        u_d['u_trans'] = u.get('trans')
        u_d['u_whoConfidence'] = u.get('whoConfidence')
        u_d['u_toks_n'] = len(list(u.iter('w')))
        utterances.append(u_d)
```

``` python
utterances = pd.DataFrame(utterances)
```

``` python
utterances
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text_id</th>
      <th>u_n</th>
      <th>u_who</th>
      <th>u_trans</th>
      <th>u_whoConfidence</th>
      <th>u_toks_n</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>SN64</td>
      <td>1</td>
      <td>S0590</td>
      <td>nonoverlap</td>
      <td>high</td>
      <td>18</td>
    </tr>
    <tr>
      <th>1</th>
      <td>SN64</td>
      <td>2</td>
      <td>S0588</td>
      <td>nonoverlap</td>
      <td>high</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>SN64</td>
      <td>3</td>
      <td>S0590</td>
      <td>nonoverlap</td>
      <td>high</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>SN64</td>
      <td>4</td>
      <td>S0588</td>
      <td>nonoverlap</td>
      <td>high</td>
      <td>9</td>
    </tr>
    <tr>
      <th>4</th>
      <td>SN64</td>
      <td>5</td>
      <td>S0589</td>
      <td>overlap</td>
      <td>high</td>
      <td>7</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1248105</th>
      <td>SMHY</td>
      <td>261</td>
      <td>S0037</td>
      <td>overlap</td>
      <td>high</td>
      <td>9</td>
    </tr>
    <tr>
      <th>1248106</th>
      <td>SMHY</td>
      <td>262</td>
      <td>S0115</td>
      <td>nonoverlap</td>
      <td>high</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1248107</th>
      <td>SMHY</td>
      <td>263</td>
      <td>S0037</td>
      <td>nonoverlap</td>
      <td>high</td>
      <td>6</td>
    </tr>
    <tr>
      <th>1248108</th>
      <td>SMHY</td>
      <td>264</td>
      <td>S0115</td>
      <td>nonoverlap</td>
      <td>high</td>
      <td>29</td>
    </tr>
    <tr>
      <th>1248109</th>
      <td>SMHY</td>
      <td>265</td>
      <td>S0037</td>
      <td>nonoverlap</td>
      <td>high</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>1248110 rows × 6 columns</p>
</div>

``` python
if not testing:
    utterances.to_csv('../out/utterances.csv', index=False)
```

## Create utterance table for annotating

For this, I use the untagged version of the corpus in the directory
`spoken/untagged/`.

``` python
path_texts_untag = list(path_corpus_untagged.glob('*.xml'))
```

``` python
texts_untag = [get_xml(fp) for fp in path_texts_untag]
```

Limit to texts that have the word ‘request’ in the `conv_type` field of
the `header` preamble.

``` python
texts_untag_requests = []

for text in texts_untag:
    header = text.find('header')
    if header is not None:
        conv_type = header.find('conv_type')
        if conv_type is not None:
            conv_type_text = conv_type.text
            if conv_type_text is not None:
                if 'request' in conv_type_text:
                    texts_untag_requests.append(text)
                else:
                    continue
            else:
                continue
        else:
            continue
    else:
        continue
```

``` python
print(
    f'all texts: {len(texts_untag)}',
    f'texts with requests: {len(texts_untag_requests)}',
    sep='\n'
)
```

    all texts: 1251
    texts with requests: 154

``` python
utterances_requests = []

for text in texts_untag_requests:
    for u in text.iter('u'):
        u_d = {}
        u_d['text_id'] = text.get('id')
        u_d['u_n'] = u.get('n')
        u_d['u_who'] = u.get('who')
        u_d['text'] = u.text
        utterances_requests.append(u_d)
```

``` python
utterances_requests = pd.DataFrame(utterances_requests)
```

``` python
utterances_requests
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text_id</th>
      <th>u_n</th>
      <th>u_who</th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>SQ2W</td>
      <td>1</td>
      <td>S0439</td>
      <td>we have to like move out of this house</td>
    </tr>
    <tr>
      <th>1</th>
      <td>SQ2W</td>
      <td>2</td>
      <td>S0441</td>
      <td>no</td>
    </tr>
    <tr>
      <th>2</th>
      <td>SQ2W</td>
      <td>3</td>
      <td>S0439</td>
      <td>no no not move out of the house we have this v...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>SQ2W</td>
      <td>4</td>
      <td>S0441</td>
      <td>None</td>
    </tr>
    <tr>
      <th>4</th>
      <td>SQ2W</td>
      <td>5</td>
      <td>S0439</td>
      <td>oh er are you here this weekend?</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>204613</th>
      <td>SJSC</td>
      <td>1166</td>
      <td>S0439</td>
      <td>mm</td>
    </tr>
    <tr>
      <th>204614</th>
      <td>SJSC</td>
      <td>1167</td>
      <td>S0440</td>
      <td>but the pension at the moment</td>
    </tr>
    <tr>
      <th>204615</th>
      <td>SJSC</td>
      <td>1168</td>
      <td>S0439</td>
      <td>we're not</td>
    </tr>
    <tr>
      <th>204616</th>
      <td>SJSC</td>
      <td>1169</td>
      <td>S0440</td>
      <td>None</td>
    </tr>
    <tr>
      <th>204617</th>
      <td>SJSC</td>
      <td>1170</td>
      <td>S0440</td>
      <td>ah bonjour Paris calling Paris calling</td>
    </tr>
  </tbody>
</table>
<p>204618 rows × 4 columns</p>
</div>

filter out utterances without text

``` python
utterances_requests = utterances_requests[utterances_requests['text'].notna()]
```

randomize rows

``` python
utterances_requests = utterances_requests.sample(frac=1).reset_index(drop=True)
```

select first 50,000 rows

``` python
utterances_requests = utterances_requests.iloc[:50_000]
```

write out to `out/utterances_requests_50k.csv`

``` python
if not testing:
    utterances_requests.to_csv(
        '../out/utterances_requests_50k.csv', index=False)
```

# Speakers

``` python
meta_speakers_head = pd.read_csv(
    fp_meta_speakers_fields,
    delimiter='\t',
    skiprows=1,
    index_col=0
)
```

``` python
meta_speakers = pd.read_csv(
    fp_meta_speakers, 
    delimiter='\t', 
    names=meta_speakers_head['XML tag'],
    index_col=0
)
```

``` python
meta_speakers
```

## Add number of tokens per speaker

``` python
speakers_toks = defaultdict(int)

for text in texts:
    for u in text.iter('u'):
        who = u.get('who')
        n_words = len([w for w in u.iter('w')])
        speakers_toks[who] += n_words
```

``` python
speaker_toks = pd.DataFrame(list(speakers_toks.items()), columns=['who', 'speaker_toks_n'])
```

``` python
speaker_toks.sort_values(by='speaker_toks_n', ascending=False).head(10)
```

``` python
meta_speakers_merge = meta_speakers.reset_index().rename(columns={'index': 'who'})
```

``` python
meta_speakers = pd.merge(
    left=meta_speakers_merge,
    right=speaker_toks,
    on='who'
)
```

``` python
meta_speakers
```

## Write out

``` python
if not testing:
    meta_speakers.to_csv('../out/speakers.csv', index=False)
```

# Tokens

In addition to the metadata present in the corpus, I’ve added the
following columns:

- `w_idx`: token position (‘index’) in the given utterance, starting at
  1
- `w_L1`: preceding token
- `w_R1`: subsequent token

``` python
tokens = []

for text in texts:
    tok_d = {}
    tok_d['text_id'] = text.get('id')

    for u in text.findall('u'):
        tok_d['u_n'] = u.get('n')

        u_toks = list(u.iter('w'))
        for i, w in enumerate(u_toks):
            tok_d['w_pos'] = w.get('pos')
            tok_d['w_lemma'] = w.get('lemma')
            tok_d['w_class'] = w.get('class')
            tok_d['w_usas'] = w.get('usas')
            tok_d['w_text'] = w.text
            tok_d['w_idx'] = i + 1
            tok_d['w_L1'] = u_toks[i-1].text if i > 0 else '<s>'
            tok_d['w_R1'] = u_toks[i+1].text if i < len(u_toks) - 1 else '</s>'

            tokens.append(tok_d.copy())
```

``` python
tokens = pd.DataFrame(tokens)
```

``` python
tokens.head(20)
```

``` python
assert len(tokens) == tokens_n
```

I export the full token table to `tokens.csv`.

``` python
if not testing:
    tokens.to_csv('../out/tokens.csv', index=False)
```

I also export a smaller version for use in spreadsheet software. This
version contains the first 50,000 tokens in the corpus and is stored in
`tokens_small.csv`.

``` python
if not testing:
    (tokens
     .head(50_000)
     .to_csv('../out/tokens_small.csv', index=False))
```

# Merge tokens with metadata

``` python
tokens.info()
```

## + utterance information

``` python
toks_utt = pd.merge(
    tokens,
    utterances,
    on = ['text_id', 'u_n']
)
```

``` python
toks_utt.info()
```

## + text information

``` python
toks_utt_text = pd.merge(
    toks_utt,
    meta_texts,
    on = 'text_id'
)
```

``` python
toks_utt_text.info()
```

## + speaker information

``` python
toks_utt_text_speakers = pd.merge(
    toks_utt_text,
    meta_speakers,
    left_on = 'u_who',
    right_on = 'who'
)
```

``` python
toks_utt_text_speakers.info()
```

## Write out

``` python
if not testing:
    toks_utt_text_speakers.to_csv('../out/tokens-plus-meta.csv', index=False)
    print(f'number of rows: {len(toks_utt_text_speakers)}')
    print(f'file size: {os.path.getsize("../out/tokens-plus-meta.csv") / 1_000_000:.2f} MB')
```

I also write out a small version containing the first 50,000 rows for
use in spreadsheet software:

``` python
if not testing:
    toks_utt_text_speakers.iloc[:50_000].to_csv(
        '../out/tokens-plus-meta_small.csv', index=False)
    print(f'number of rows: {len(toks_utt_text_speakers.iloc[:50_000])}')
    print(f'file size: {os.path.getsize("../out/tokens-plus-meta_small.csv") / 1_000_000:.2f} MB')
```
