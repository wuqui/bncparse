{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BNCparse\n",
    "\n",
    "> Parsing the BNC2014 Spoken with Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quirin Würschinger, LMU Munich\n",
    "\n",
    "[q.wuerschinger@lmu.de](mailto:q.wuerschinger@lmu.de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation: <https://wuqui.github.io/bncparse/>\n",
    "\n",
    "Please visit the above website, as GitHub cannot render everything properly in the version below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagram below illustrates all of the data that is currently available. Variables that have been added to what was available from the downloadable version of the BNC are marked with a `+` prefix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{mermaid}\n",
    "%%| fig-width: 7\n",
    "classDiagram\n",
    "\n",
    "class text {\n",
    "    <<conversation>>\n",
    "    text_id : \"Text ID\"\n",
    "}\n",
    "\n",
    "class u {\n",
    "    <<utterances.csv>>\n",
    "    n : \"Consecutive utterance number\"\n",
    "    who : \"Speaker ID\"\n",
    "    trans : \"Transition type\"\n",
    "    whoConfidence: \"Attribution confidence\"\n",
    "\n",
    "    + u_toks_n : \"Number of tokens in the utterance\"\n",
    "}\n",
    "\n",
    "class w {\n",
    "    <<tokens.csv>>\n",
    "    pos : \"part-of-speech tag [CLAWS]\"\n",
    "    lemma : \"lemmatised form\"\n",
    "    class : \"“simple” POS tag or major word-class\"\n",
    "    usas : \"semantic tag [USAS]\"\n",
    "\n",
    "    + w_idx : \"token position in the given utterance\"\n",
    "    + w_idx_rel : \"relative token position in the given utterance\"\n",
    "    + w_L1 : \"preceding token\"\n",
    "    + w_R1 : = \"subsequent token\n",
    "}\n",
    "\n",
    "class meta_speaker {\n",
    "    <<speakers.csv>>\n",
    "    id : \"Speaker ID\"\n",
    "    exactage : \"Exact age\"\n",
    "    age1994 : \"Age [BNC1994 groups]\"\n",
    "    agerange : \"Age range\"\n",
    "    gender : \"Gender\"\n",
    "    nat : \"Nationality\"\n",
    "    birthplace : \"Place of birth\"\n",
    "    birthcountry : \"Country of birth\"\n",
    "    l1 : \"First language\"\n",
    "    lingorig : \"Linguistic origin\"\n",
    "    dialect_rep : \"Accent/dialect as reported\"\n",
    "    hab_city : \"City/town living\"\n",
    "    hab_country : \"Country living\"\n",
    "    hab_dur : \"Duration living [years]\"\n",
    "    dialect_l1 : \"Dialect at Level 1\"\n",
    "    dialect_l2 : \"Dialect at Level 2\"\n",
    "    dialect_l3 : \"Dialect at Level 3\"\n",
    "    dialect_l4 : \"Dialect at Level 4\"\n",
    "    edqual : \"Highest qualification\"\n",
    "    occupation : \"Occupation: title\"\n",
    "    socgrade : \"Class: Social grade\"\n",
    "    nssec : \"Class: NS-SEC\"\n",
    "    l2 : \"L2 [if bilingual]\"\n",
    "    fls : \"Foreign languages spoken\"\n",
    "    in_core : \"Part of core set of speakers\"\n",
    "    + speaker_toks_n : \"Total number of tokens\"\n",
    "}\n",
    "\n",
    "class meta_text {\n",
    "    <<texts.csv>>\n",
    "    text_id : \"Text ID\"\n",
    "    rec_length : \"Recording length\"\n",
    "    rec_date : \"Recording date\"\n",
    "    rec_year : \"Year of recording\"\n",
    "    rec_period : \"Recording period\"\n",
    "    n_speakers : \"Number of speakers\"\n",
    "    list_speakers : \"List of speaker IDs\"\n",
    "    rec_loc : \"Recording location\"\n",
    "    relationships : \"Inter-speaker relationship\"\n",
    "    topics : \"Topics covered\"\n",
    "    activity : \"Activity description\"\n",
    "    conv_type : \"Selected characterisations of conversation type\"\n",
    "    conventions : \"Transcription conventions used\"\n",
    "    in_sample : \"Sample release inclusion\"\n",
    "    transcriber : \"Transcriber\"\n",
    "}\n",
    "\n",
    "text ..* u : contains\n",
    "u ..* w : contains\n",
    "text .. meta_text : text_id\n",
    "u .. meta_speaker : who\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load packages\n",
    "\n",
    "Package requirements are stored in `requirements.yml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "from lxml import etree\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For development, I use a small subset of the corpus contained in `data/test` that only contains the first 10 texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = True\n",
    "\n",
    "if testing:\n",
    "    path_bnc = Path('../data/test/bnc-2014-spoken')\n",
    "    assert path_bnc.exists()\n",
    "    texts_n = 10\n",
    "    tokens_n = 94_659\n",
    "else:\n",
    "    path_bnc = Path('../data/bnc-2014-spoken')\n",
    "    assert path_bnc.exists()\n",
    "    texts_n = 1251\n",
    "    tokens_n = 11_422_615"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_corpus = Path(path_bnc / 'spoken' / 'tagged')\n",
    "path_corpus_untagged = Path(path_bnc / 'spoken' / 'untagged')\n",
    "path_metadata = Path(path_bnc / 'spoken' / 'metadata')\n",
    "fp_meta_speakers = Path('../data/bnc-2014-spoken/spoken/metadata/bnc2014spoken-speakerdata.tsv')\n",
    "fp_meta_speakers_fields = Path('../data/bnc-2014-spoken/spoken/metadata/metadata-fields-speaker.txt')\n",
    "fp_meta_texts = Path('../data/bnc-2014-spoken/spoken/metadata/bnc2014spoken-textdata.tsv')\n",
    "fp_meta_texts_fields = Path('../data/bnc-2014-spoken/spoken/metadata/metadata-fields-text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert path_corpus.exists()\n",
    "assert path_corpus_untagged.exists()\n",
    "assert path_metadata.exists()\n",
    "assert fp_meta_speakers.exists()\n",
    "assert fp_meta_speakers_fields.exists()\n",
    "assert fp_meta_texts.exists()\n",
    "assert fp_meta_texts_fields.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and parse XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_texts = list(path_corpus.glob('*.xml'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(path_texts) == texts_n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_xml(f_path):\n",
    "    with open(f_path, 'r') as f:\n",
    "        f = f.read()\n",
    "    xml = etree.fromstring(f)\n",
    "    return xml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [get_xml(path) for path in path_texts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_texts_head = pd.read_csv(\n",
    "    fp_meta_texts_fields,\n",
    "    delimiter='\\t',\n",
    "    skiprows=1,\n",
    "    index_col=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_texts = pd.read_csv(\n",
    "    fp_meta_texts, \n",
    "    delimiter='\\t', \n",
    "    names=meta_texts_head['XML tag'],\n",
    "    index_col=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add number of tokens per text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_tokens = []\n",
    "\n",
    "for text in texts:\n",
    "    text_d = {}\n",
    "    text_d['text_id'] = text.get('id')\n",
    "    text_d['text_toks_n'] = 0\n",
    "    for tok in text.iter('w'):\n",
    "        text_d['text_toks_n'] += 1\n",
    "    texts_tokens.append(text_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_tokens = pd.DataFrame(texts_tokens)\n",
    "texts_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index and call it text_id\n",
    "meta_texts_merge = meta_texts.reset_index().rename(columns={'index': 'text_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_texts = pd.merge(\n",
    "    left=meta_texts_merge,\n",
    "    right=texts_tokens,\n",
    "    on='text_id'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not testing:\n",
    "    meta_texts.to_csv('../out/texts.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = []\n",
    "\n",
    "for text in texts:\n",
    "    for u in text.findall('u'):\n",
    "        u_d = {}\n",
    "        u_d['text_id'] = text.get('id')\n",
    "        u_d['u_n'] = u.get('n')\n",
    "        u_d['u_who'] = u.get('who')\n",
    "        u_d['u_trans'] = u.get('trans')\n",
    "        u_d['u_whoConfidence'] = u.get('whoConfidence')\n",
    "        u_d['u_toks_n'] = len(list(u.iter('w')))\n",
    "        utterances.append(u_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = pd.DataFrame(utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>u_n</th>\n",
       "      <th>u_who</th>\n",
       "      <th>u_trans</th>\n",
       "      <th>u_whoConfidence</th>\n",
       "      <th>u_toks_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SN64</td>\n",
       "      <td>1</td>\n",
       "      <td>S0590</td>\n",
       "      <td>nonoverlap</td>\n",
       "      <td>high</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SN64</td>\n",
       "      <td>2</td>\n",
       "      <td>S0588</td>\n",
       "      <td>nonoverlap</td>\n",
       "      <td>high</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SN64</td>\n",
       "      <td>3</td>\n",
       "      <td>S0590</td>\n",
       "      <td>nonoverlap</td>\n",
       "      <td>high</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SN64</td>\n",
       "      <td>4</td>\n",
       "      <td>S0588</td>\n",
       "      <td>nonoverlap</td>\n",
       "      <td>high</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SN64</td>\n",
       "      <td>5</td>\n",
       "      <td>S0589</td>\n",
       "      <td>overlap</td>\n",
       "      <td>high</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248105</th>\n",
       "      <td>SMHY</td>\n",
       "      <td>261</td>\n",
       "      <td>S0037</td>\n",
       "      <td>overlap</td>\n",
       "      <td>high</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248106</th>\n",
       "      <td>SMHY</td>\n",
       "      <td>262</td>\n",
       "      <td>S0115</td>\n",
       "      <td>nonoverlap</td>\n",
       "      <td>high</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248107</th>\n",
       "      <td>SMHY</td>\n",
       "      <td>263</td>\n",
       "      <td>S0037</td>\n",
       "      <td>nonoverlap</td>\n",
       "      <td>high</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248108</th>\n",
       "      <td>SMHY</td>\n",
       "      <td>264</td>\n",
       "      <td>S0115</td>\n",
       "      <td>nonoverlap</td>\n",
       "      <td>high</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248109</th>\n",
       "      <td>SMHY</td>\n",
       "      <td>265</td>\n",
       "      <td>S0037</td>\n",
       "      <td>nonoverlap</td>\n",
       "      <td>high</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1248110 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id  u_n  u_who     u_trans u_whoConfidence  u_toks_n\n",
       "0          SN64    1  S0590  nonoverlap            high        18\n",
       "1          SN64    2  S0588  nonoverlap            high         0\n",
       "2          SN64    3  S0590  nonoverlap            high         1\n",
       "3          SN64    4  S0588  nonoverlap            high         9\n",
       "4          SN64    5  S0589     overlap            high         7\n",
       "...         ...  ...    ...         ...             ...       ...\n",
       "1248105    SMHY  261  S0037     overlap            high         9\n",
       "1248106    SMHY  262  S0115  nonoverlap            high         2\n",
       "1248107    SMHY  263  S0037  nonoverlap            high         6\n",
       "1248108    SMHY  264  S0115  nonoverlap            high        29\n",
       "1248109    SMHY  265  S0037  nonoverlap            high         1\n",
       "\n",
       "[1248110 rows x 6 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not testing:\n",
    "    utterances.to_csv('../out/utterances.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create utterance table for annotating"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this, I use the untagged version of the corpus in the directory `spoken/untagged/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_texts_untag = list(path_corpus_untagged.glob('*.xml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_untag = [get_xml(fp) for fp in path_texts_untag]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit to texts that have the word 'request' in the `conv_type` field of the `header` preamble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_untag_requests = []\n",
    "\n",
    "for text in texts_untag:\n",
    "    header = text.find('header')\n",
    "    if header is not None:\n",
    "        conv_type = header.find('conv_type')\n",
    "        if conv_type is not None:\n",
    "            conv_type_text = conv_type.text\n",
    "            if conv_type_text is not None:\n",
    "                if 'request' in conv_type_text:\n",
    "                    texts_untag_requests.append(text)\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all texts: 1251\n",
      "texts with requests: 154\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "\tf'all texts: {len(texts_untag)}',\n",
    "\tf'texts with requests: {len(texts_untag_requests)}',\n",
    "\tsep='\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances_requests = []\n",
    "\n",
    "for text in texts_untag_requests:\n",
    "    for u in text.iter('u'):\n",
    "        u_d = {}\n",
    "        u_d['text_id'] = text.get('id')\n",
    "        u_d['u_n'] = u.get('n')\n",
    "        u_d['u_who'] = u.get('who')\n",
    "        u_d['text'] = u.text\n",
    "        utterances_requests.append(u_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances_requests = pd.DataFrame(utterances_requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>u_n</th>\n",
       "      <th>u_who</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SQ2W</td>\n",
       "      <td>1</td>\n",
       "      <td>S0439</td>\n",
       "      <td>we have to like move out of this house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SQ2W</td>\n",
       "      <td>2</td>\n",
       "      <td>S0441</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SQ2W</td>\n",
       "      <td>3</td>\n",
       "      <td>S0439</td>\n",
       "      <td>no no not move out of the house we have this v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SQ2W</td>\n",
       "      <td>4</td>\n",
       "      <td>S0441</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SQ2W</td>\n",
       "      <td>5</td>\n",
       "      <td>S0439</td>\n",
       "      <td>oh er are you here this weekend?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204613</th>\n",
       "      <td>SJSC</td>\n",
       "      <td>1166</td>\n",
       "      <td>S0439</td>\n",
       "      <td>mm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204614</th>\n",
       "      <td>SJSC</td>\n",
       "      <td>1167</td>\n",
       "      <td>S0440</td>\n",
       "      <td>but the pension at the moment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204615</th>\n",
       "      <td>SJSC</td>\n",
       "      <td>1168</td>\n",
       "      <td>S0439</td>\n",
       "      <td>we're not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204616</th>\n",
       "      <td>SJSC</td>\n",
       "      <td>1169</td>\n",
       "      <td>S0440</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204617</th>\n",
       "      <td>SJSC</td>\n",
       "      <td>1170</td>\n",
       "      <td>S0440</td>\n",
       "      <td>ah bonjour Paris calling Paris calling</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>204618 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       text_id   u_n  u_who                                               text\n",
       "0         SQ2W     1  S0439             we have to like move out of this house\n",
       "1         SQ2W     2  S0441                                                no \n",
       "2         SQ2W     3  S0439  no no not move out of the house we have this v...\n",
       "3         SQ2W     4  S0441                                               None\n",
       "4         SQ2W     5  S0439                   oh er are you here this weekend?\n",
       "...        ...   ...    ...                                                ...\n",
       "204613    SJSC  1166  S0439                                                mm \n",
       "204614    SJSC  1167  S0440                      but the pension at the moment\n",
       "204615    SJSC  1168  S0439                                         we're not \n",
       "204616    SJSC  1169  S0440                                               None\n",
       "204617    SJSC  1170  S0440             ah bonjour Paris calling Paris calling\n",
       "\n",
       "[204618 rows x 4 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterances_requests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter out utterances without text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances_requests = utterances_requests[utterances_requests['text'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "randomize rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances_requests = utterances_requests.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select first 50,000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances_requests = utterances_requests.iloc[:50_000]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write out to `out/utterances_requests_50k.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not testing:\n",
    "    utterances_requests.to_csv(\n",
    "        '../out/utterances_requests_50k.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_speakers_head = pd.read_csv(\n",
    "    fp_meta_speakers_fields,\n",
    "    delimiter='\\t',\n",
    "    skiprows=1,\n",
    "    index_col=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_speakers = pd.read_csv(\n",
    "    fp_meta_speakers, \n",
    "    delimiter='\\t', \n",
    "    names=meta_speakers_head['XML tag'],\n",
    "    index_col=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_speakers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add number of tokens per speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers_toks = defaultdict(int)\n",
    "\n",
    "for text in texts:\n",
    "    for u in text.iter('u'):\n",
    "        who = u.get('who')\n",
    "        n_words = len([w for w in u.iter('w')])\n",
    "        speakers_toks[who] += n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_toks = pd.DataFrame(list(speakers_toks.items()), columns=['who', 'speaker_toks_n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_toks.sort_values(by='speaker_toks_n', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_speakers_merge = meta_speakers.reset_index().rename(columns={'index': 'who'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_speakers = pd.merge(\n",
    "    left=meta_speakers_merge,\n",
    "    right=speaker_toks,\n",
    "    on='who'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_speakers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not testing:\n",
    "    meta_speakers.to_csv('../out/speakers.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the metadata present in the corpus, I’ve added the following columns:\n",
    "\n",
    "- `w_idx`: token position (‘index’) in the given utterance, starting at 1\n",
    "- `w_L1`: preceding token\n",
    "- `w_R1`: subsequent token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "\n",
    "for text in texts:\n",
    "    tok_d = {}\n",
    "    tok_d['text_id'] = text.get('id')\n",
    "\n",
    "    for u in text.findall('u'):\n",
    "        tok_d['u_n'] = u.get('n')\n",
    "\n",
    "        u_toks = list(u.iter('w'))\n",
    "        for i, w in enumerate(u_toks):\n",
    "            tok_d['w_pos'] = w.get('pos')\n",
    "            tok_d['w_lemma'] = w.get('lemma')\n",
    "            tok_d['w_class'] = w.get('class')\n",
    "            tok_d['w_usas'] = w.get('usas')\n",
    "            tok_d['w_text'] = w.text\n",
    "            tok_d['w_idx'] = i + 1\n",
    "            tok_d['w_L1'] = u_toks[i-1].text if i > 0 else '<s>'\n",
    "            tok_d['w_R1'] = u_toks[i+1].text if i < len(u_toks) - 1 else '</s>'\n",
    "\n",
    "            tokens.append(tok_d.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = pd.DataFrame(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(tokens) == tokens_n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I export the full token table to `tokens.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not testing:\n",
    "    tokens.to_csv('../out/tokens.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also export a smaller version for use in spreadsheet software. This version contains the first 50,000 tokens in the corpus and is stored in `tokens_small.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not testing:\n",
    "    (tokens\n",
    "     .head(50_000)\n",
    "     .to_csv('../out/tokens_small.csv', index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge tokens with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## + utterance information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks_utt = pd.merge(\n",
    "    tokens,\n",
    "    utterances,\n",
    "    on = ['text_id', 'u_n']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks_utt.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## + text information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks_utt_text = pd.merge(\n",
    "    toks_utt,\n",
    "    meta_texts,\n",
    "    on = 'text_id'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks_utt_text.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## + speaker information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks_utt_text_speakers = pd.merge(\n",
    "    toks_utt_text,\n",
    "    meta_speakers,\n",
    "    left_on = 'u_who',\n",
    "    right_on = 'who'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks_utt_text_speakers.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not testing:\n",
    "    toks_utt_text_speakers.to_csv('../out/tokens-plus-meta.csv', index=False)\n",
    "    print(f'number of rows: {len(toks_utt_text_speakers)}')\n",
    "    print(f'file size: {os.path.getsize(\"../out/tokens-plus-meta.csv\") / 1_000_000:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also write out a small version containing the first 50,000 rows for use in spreadsheet software:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not testing:\n",
    "    toks_utt_text_speakers.iloc[:50_000].to_csv(\n",
    "        '../out/tokens-plus-meta_small.csv', index=False)\n",
    "    print(f'number of rows: {len(toks_utt_text_speakers.iloc[:50_000])}')\n",
    "    print(f'file size: {os.path.getsize(\"../out/tokens-plus-meta_small.csv\") / 1_000_000:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('bncparse')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
