{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BNCparse\n",
    "\n",
    "> Parsing the BNC2014 Spoken with Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quirin Würschinger, LMU Munich\n",
    "\n",
    "[q.wuerschinger@lmu.de](mailto:q.wuerschinger@lmu.de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load packages\n",
    "\n",
    "Package requirements are stored in `requirements.yml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "from lxml import etree\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BNC2014 needs to be downloaded for this script to work. It can be obtained from the official [BNC website](http://corpora.lancs.ac.uk/bnc2014/). \n",
    "\n",
    "The following variables need to be updated to the corpus’ local path. In the current setting the BNC2014 data were stored in the project folder in the folder `data/bnc-2014-spoken`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For development, I use a small subset of the corpus contained in `data/test` that only contains the first 10 texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test version\n",
    "path_bnc = Path('../data/test/bnc-2014-spoken')\n",
    "texts_n = 10\n",
    "tokens_n = 94659\n",
    "\n",
    "# full version\n",
    "# path_bnc = Path('../data/bnc-2014-spoken')\n",
    "# texts_n = 10\n",
    "# tokens_n = 94659"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_corpus = Path(path_bnc / 'spoken' / 'tagged')\n",
    "path_metadata = Path(path_bnc / 'spoken' / 'metadata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert path_bnc.exists()\n",
    "assert path_corpus.exists()\n",
    "assert path_metadata.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and parse XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_texts = list(path_corpus.glob('*.xml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(path_texts) == texts_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_xml(f_path):\n",
    "    with open(f_path, 'r') as f:\n",
    "        f = f.read()\n",
    "    xml = etree.fromstring(f)\n",
    "    return xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [get_xml(path) for path in path_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Texts\n",
    "\n",
    "Calculate the total number of texts in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ids = [xml.get('id') for xml in texts]\n",
    "\n",
    "print(f\"number of documents in the corpus: {len(text_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(text_ids) == texts_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speakers\n",
    "\n",
    "1. Determine all speakers in the corpus.\n",
    "2. Calculate the total number of words each speaker has contributed to the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers_words = defaultdict(int)\n",
    "for text in texts:\n",
    "    for u in text.iter('u'):\n",
    "        speaker = u.get('who')\n",
    "        n_words = len([w for w in u.iter('w')])\n",
    "        speakers_words[speaker] += n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"number of speakers: {len(speakers_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words per speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speakers_tokens = pd.DataFrame(list(speakers_words.items()), columns=['speaker', 'tokens'])\n",
    "df_speakers_tokens = df_speakers_tokens.sort_values('tokens', ascending=False)\n",
    "df_speakers_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table containing all speakers and their total token counts can be found in `speakers_tokens.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speakers_tokens.to_csv('../out/speakers_tokens.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for text in texts:\n",
    "    for w in text.iter('w'):\n",
    "        tokens.append(w.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([\n",
    "\t['tokens', f'{len(tokens):,}'],\n",
    "\t['types', f'{len(set(tokens)):,}'],\n",
    "\t]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export corpus data in tabular format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tokens = []\n",
    "\n",
    "for text in texts:\n",
    "    for u in text.findall('u'):\n",
    "        for i, w in enumerate(u.iter('w')):\n",
    "            tok_d = {}\n",
    "\n",
    "            tok_d['text_id'] = text.get('id')\n",
    "\n",
    "            tok_d['u_n'] = u.get('n')\n",
    "            tok_d['u_who'] = u.get('who')\n",
    "            tok_d['u_trans'] = u.get('trans')\n",
    "            tok_d['u_whoConfidence'] = u.get('whoConfidence')\n",
    "\n",
    "            tok_d['w_pos'] = w.get('pos')\n",
    "            tok_d['w_lemma'] = w.get('lemma')\n",
    "            tok_d['w_class'] = w.get('class')\n",
    "            tok_d['w_usas'] = w.get('usas')\n",
    "            tok_d['w_text'] = w.text\n",
    "            tok_d['w_idx'] = i + 1\n",
    "\n",
    "            tokens.append(tok_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tokens = pd.DataFrame(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(tokens) == tokens_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I export the full token table to `tokens.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.to_csv('../out/tokens.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also export a smaller version for use in spreadsheet software. This version contains the first 50,000 tokens in the corpus and is stored in `tokens_50k.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out first 50000 rows of `tokens`\n",
    "(tokens\n",
    " .head(50_000)\n",
    " .to_csv('../out/tokens_50k.csv', index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('bncparse')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
